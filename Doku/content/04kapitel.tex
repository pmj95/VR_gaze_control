%!TEX root = ../dokumentation.tex

\chapter{Versuchsumgebung}
\label{section:versuchsumgebung}
\todo[inline]{Gaze Data Logging hier und Kap 5}

Im folgenden Kapitel wird die Versuchsumgebung erläutert. Um einen Überblick über die Versuchsumgebung zu bekommen, wird zu Beginn der Aufbau der Umgebungen beschrieben. Danach werden die einzelnen Szenen vorgestellt, welche für die Versuchsdurchführung erstellt wurden. In dem Kapitel werden zudem die verschiedenen Steuerungsmöglichkeiten für die Versuche vorgestellt. Als letztes wird die Implementierung erläutert. 

\section{Aufbau}
Um die Verwendung von Eye-Tracking zur Steuerung von Bedienelementen in \ac{VR} untersuchen zu können, finden die Versuche in einem leeren, neutralen Raum statt. Ein genereller Vorteil von Versuchen in \ac{VR} ist, dass die Versuche in einer digitalen Umgebung stattfinden. Dadurch wird die komplette Kontrolle über die Umgebung behalten und die Versuche sind stets wiederholbar. Der Versuch im leeren neutralen Raum hat den Vorteil, dass der Benutzer von der kompletten realen Welt abgeschirmt ist. Ein Nebeneffekt dabei ist, dass der Benutzer sich besser auf den Versuch fokussieren kann, da dieser nicht durch eventuell neu gewonnene Eindrücke aus der \ac{VR}-Welt abgelenkt wird. Um Ablenkungen bezüglich Farbwechsel an den Wänden, dem Boden oder der Decke zu vermeiden, erhalten diese Elemente die selbe Hintergrundfarbe. Zudem wird der Raum gleichmäßig beleuchtet. Für eine bessere Vergleichbarkeit der Versuchsergebnisse muss der Benutzer bei jedem Versuch auf der gleichen Position im Raum stehen. Diese Position wird auf dem Boden durch ein rotes Quadrat markiert (siehe \autoref{fig:game-plan}). Da der Benutzer die Möglichkeit hat, sich innerhalb einer vom \ac{VR}-Headset berechneten Spielfläche zu bewegen, muss sich der Benutzer vor dem Beginn des Versuches auf dem roten Quadrat positionieren. Dieses Quadrat befindet sich mittig zentriert circa eine halbe Einheit vor einer Wand. Das Quadrat hat eine Seitenlänge von einer halben Einheit.

Zur Untersuchung der Eignung von Eye-Tracking bei der Steuerung von Bedienelementen befindet sich auf der gegenüberliegenden Wand die Versuchsfläche (siehe \autoref{fig:game-view}). Der Versuch ist wie ein Spiel aufgebaut. Der Benutzer muss fünf zufällige \todo{pseudorandom, nicht random, da immer die gleichen distanzen abgedeckt werden (z.b. diagonalen usw.); Naja stand jetzt nur in Fitts Law} Zahlen von 1 bis 16 nacheinander auswählen. Auf der Spielfläche befinden sich 16 Bedienelemente, die mit Zahlen von 1 bis 16 beschriftet sind. Oberhalb der Bedienelemente befindet sich ein Textfeld, welches dem Benutzer die auszuwählende Zahl mitteilt. Zudem teilt das Textfeld das Ende des Spieles mit. Um das Spiel zu starten, muss der Benutzer den GO-Button betätigen. Beim Betätigen eines Bedienelementes wird die Hintergrundfarbe des Elements verändert. Dies wird in Abhängigkeit der gesuchten Zahl und des ausgewählten Elements festgelegt. Wenn die gesuchte Zahl und die Beschriftung des ausgewählten Bedienelements übereinstimmen, wird das Element grün gefärbt, ansonsten rot. \\
Für die Untersuchung von Fitts's Law in \ac{VR} sind die Bedienelemente im Durchmesser verstellbar. Für die Versuche stehen drei verschiedene Skalierungen zur Verfügung. Der größte Durchmesser ist 0,4 Einheiten groß und der Skalierungsfaktor ist eins. Der mittlere Durchmesser wird mit dreiviertel des größten Durchmesser skaliert. Der kleinste Durchmesser ist nur noch halb so groß wie der größte. \todo{hier den skalikerungsfaktor nennen, da das eine Aussage über die Veränderung der breite gibt und das bei Fitts law einer der zwei hauptaspekte ist} 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.65\linewidth]{game-plan}
	\caption[Draufsicht auf den Raum]{Draufsicht auf den Raum}
	\label{fig:game-plan}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{game-view}
\caption[Benutzersicht auf den Versuch]{Benutzersicht auf den Versuch}
\label{fig:game-view}
\end{figure}

\section{Szenen}
\todo{@Clemens: Erklären wieso die Szenen so aufgebaut wurden? Z.B. warum verschiedene Entfernungen? Und wieso das 3D-Level? passt soweit?}
\todo{@Clemens: Bezug auf Forschungsfrage! passt soweit?}
Für die Versuche existieren vier verschiedene Szenen. Jede Szene basiert auf dem zuvor beschriebenen Aufbau. Jeder der Räume ist 2,5 Einheiten hoch und 5 Einheiten breit. Die Idee an den ersten drei Szenen ist zu untersuchen, welchen Einfluss die Entfernung der Bedienelemente auf die Zuverlässigkeit des Eye-Trackings hat\todo{und auf Fitts Law; @Clemens: die Frage ist doch ob es der nächste Satz nicht schon aussagt??}. Zudem wird in diesen drei Szenen Fitts' Gesetz in \ac{VR} mit Eye-Tracking untersucht. Die Namen der Szenen orientieren sich daher an Fitts's Law. In der ersten Szene \glqq FittsLaw\grqq (siehe \autoref{fig:game-view}) steht der Benutzer in einer Entfernung von 4,5 Einheiten zur Spielfläche. In der zweiten Szene \glqq FittsLawFar\grqq (siehe \autoref{fig:FittsLawFar}) und der dritten Szene \glqq FittsLawFurther\grqq (siehe \autoref{fig:FittsLawFurther}) vergrößert sich die Entfernung jeweils um 2,5 Einheiten zur vorherigen Szene. In der dritten Szene ist der Benutzer daher 9,5 Einheiten von der Spielfläche entfernt. 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{FittsLawFar}
	\caption[Szene FittsLawFar]{Szene FittsLawFar}
	\label{fig:FittsLawFar}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{FittsLawFurther}
	\caption[Szene FittsLawFurther]{Szene FittsLawFurther}
	\label{fig:FittsLawFurther}
\end{figure}

Bei den ersten drei Szenen ist die Spielfläche zweidimensional. Die Spielfläche befindet sich an der Wand und die Bedienelemente befinden sich auf der gleichen Ebene an der Wand. In der vierten und letzten Szene \glqq 3D-Level\grqq{} (siehe \autoref{fig:3D-Level})\todo{Komma um 3D Level? bin mir aber nicht sicher; @Clemens: gute Frage. Kommasetzung ist überhaupt nicht meins xD} wird untersucht, welche Auswirkungen eine dreidimensionale Spielfläche auf das Eye-Tracking hat. Die Bedienelemente befinden sich nicht mehr auf der gleichen Ebene, sondern schweben im Raum. Die Entfernung zwischen dem Benutzer und den Bedienelementen variiert von 4,5 bis 9,5 Einheiten. Der interessante Aspekt an dieser Szene ist, dass gegebenenfalls ein Bedienelement zum Teil ein anderes Bedienelement verdeckt. Hierbei wird insbesondere die Zuverlässigkeit des Eye-Trackings auf die Probe gestellt. 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{3dlevel}
	\caption[Szene 3D-Level]{Szene 3D-Level}
	\label{fig:3D-Level}
\end{figure}

\section{Steuerung}
\todo{WICHTIG: Klären wie die Steuerungsarten benannt und geschrieben werden. Gilt insbesondere für Kapitel 5}
Um beurteilen zu können, ob sich Eye-Tracking zur Steuerung von Bedienelementen in \ac{VR} eignet, werden verschiedene Steuerungsmöglichkeiten getestet. Die Steuerung innerhalb der \ac{VR}-Umgebung kann mit den Controllern erfolgen und mit dem Eye-Tracking. Die Steuerung lässt sich in Anvisieren und Interagieren unterteilen. Damit beim Anvisieren von Elementen mit dem Controller für den Benutzer klar wird, in welche Richtung der Controller zeigt, wird für das Anvisieren der Controller mit einem Laser versehen. Beim Eye-Tracking wird der anvisierte Punkt durch eine kleine schwarze Kugel gekennzeichnet. Das Interagieren mit dem Controller erfolgt durch den Abzug auf der Unterseite des Controllers. Beim Eye-Tracking erfolgt dies durch Blinzeln.\\
Anhand dieser Kenntnisse lassen sich die Steuerungsmöglichkeiten in drei Gruppen aufteilen. Die erste Gruppe ist die Steuerung mit den \ac{VR}-Headset Controllern. Die Controllervariante ist die klassische Steuerungsmöglichkeit. In der Regel benötigt der Benutzer zum Interagieren mit der \ac{VR}-Umgebung mindestens einen Controller. Die zweite Gruppe ist die Steuerung mithilfe des Eye-Trackers. Bei Eye-Tracking wird der Blick zum Anvisieren verwendet und das Blinzeln zum Interagieren mit der \ac{VR}-Umgebung verwendet. Die letzte Möglichkeit ist der Mix aus der klassischen Steuerung sowie der Steuerung mithilfe des Eye-Trackings. Hier übernimmt jeweils eine Steuerungsmöglichkeit das Anvisieren und die andere das Interagieren. Daraus ergeben sich die folgenden vier Versuchskombinationen:

\begin{enumerate}
	\item \textbf{Laser - Trigger}: Diese Versuchskombination verwendet die klassische Steuerelemente in \ac{VR}. Um einschätzen zu können, wie gut Eye-Tracking im Vergleich zu der klassischen Steuerungsvariante funktioniert, wird diese Versuchskombination benötigt. 
	\item \textbf{Blickerfassung - Blink Detection}: Bei dieser Versuchskombination wird der Benutzer nur über Eye-Tracking mit der \ac{VR}-Umgebung interagieren. 
	\item \textbf{Laser - Blink Detection}: Diese Versuchskombination hilft beim Feststellen, wie gut die Interaktion in \ac{VR} mit Blinzeln funktioniert. 
	\item \textbf{Blickerfassung - Trigger}: Bei dieser Versuchskombination liegt der Fokus auf der Blickerfassung. Der Benutzer muss sich nur auf das Anvisieren eines Bedienelementes konzentrieren und mit dem Trigger am Controller die Auswahl betätigen.
\end{enumerate}

\section{Implementierung}
%\begin{wrapfigure}{r}{0.4\textwidth}
%	\vspace{-20pt}
%	\centering
%	\includegraphics{scenes-structure}
%	\vspace{0pt}
%	\caption[Aufbau der Szenen]{Aufbau der Szenen}
%	\label{fig:scenes-structure}
%	\vspace{-10pt}
%\end{wrapfigure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.4\linewidth]{scenes-structure}
	\caption[Aufbau der Szenen]{Aufbau der Szenen}
	\label{fig:scenes-structure}
\end{figure}

Die \autoref{fig:scenes-structure} zeigt exemplarisch die Objektstruktur der Szenen in Unity. Jede Szene hat 11 Hauptobjekte, welchen teilweise wieder Unterobjekte zugeordnet sind. In der folgenden Aufzählung wird jedes dieser elf Hauptobjekte kurz beschrieben. \todo{macht glaube ich mehr Sinn nur die wichtigsten kurz zu beschreiben}

\begin{description}
	\item \textbf{Wall Game}: Das Objekt ist die Wand gegenüber des Spielers. Diese Wand ist die Spielfläche, der die Buttons der Wand als Unterobjekte zugeordnet sind.
	\item \textbf{Wall right}: Das Objekt ist die Wand auf der rechten Seite des Spielers, wenn dieser sein Blick zur Spielfläche wendet. Dieser Wand ist der GO-Button zugeordnet.
	\item \textbf{Wall left}: Dieses Objekt ist die Wand auf der linken Seite des Spielers.
	\item \textbf{Wall behind}: Dieses Objekt stellt die Wand hinter dem Spieler dar.
	\item \textbf{Ground}: Das Objekt ist der Boden. Als Unterobjekt ist das rote Quadrat enthalten, auf dem sich der Spieler vor den Versuchen zu positionieren hat.
	\item \textbf{Ceiling}: Das Objekt ist die Decke.
	\item \textbf{Gaze Tracker}: Der GazeTracker enthält die relevanten Informationen für das Eye-Tracking. In diesem Objekt wird zudem die Kommunikation zum Eyetracker von Pupil Labs hergestellt. Zudem ist der Gaze Visualizer enthalten, der den anvisierten Punkt markiert.
	\item \textbf{Player}: Das Spielerobjekt enthält die relevanten Informationen, welche für den Spieler wichtig sind. Dieses Objekt implementiert das Spieler-Modell von SteamVR, welches eine einfache Integration von VR in Unity ermöglicht.
	\item \textbf{Lightning}: Das Objekt beinhaltet die Objekte für die Beleuchtung für den Raum. Für eine angenehme, gleichmäßige Beleuchtung im Raum werden zwei Point Lights verwendet. Die Lichter sind in der Mitte des Raumes jeweils einmal an der Decke und an dem Boden angebracht. 
	\item \textbf{GameFittsLaw}: Dieses Objekt beinhaltet die Spiellogik. Es ist daher nicht sichtbar in der \ac{VR}-Umgebung.
	\item \textbf{UIElements}: Diesem Objekt sind die graphischen Elemente für die verschiedenen Einstellungen für die Versuche zugeordnet. Diese Elemente sind nur in der Unity-Übersicht sichtbar. 
\end{description}

In \autoref{fig:ClassDiagram} ist ein Überblick der für dieses Projekt generierten Klassen und der Hierarchie dargestellt. Gut zu sehen ist, dass durch Vererbung der Großteil der Klassen die Unity eigene Klasse MonoBehaviour ist. Nachfolgend werden die wichtigsten Objekte beziehungsweise Strukturen erläutert. 

\subsection{UIElements}
Damit für jede Versuchskombination nicht eine eigene Umgebung erstellt werden muss, erhält jede Umgebung eine Einstellungsmöglichkeit für die verschiedenen Versuchskombinationen. In \autoref{fig:switch-different-options} ist die Einstellungsoberfläche für die Versuchskombinationen dargestellt. Für die Versuche können sowohl die Steuerungskombinationen als auch die Durchmesser der Buttons verändert werden. Zur Fixierung eines Objektes kann entweder der Laser oder das Eye-Tracking verwendet werden. Die Bestätigung kann entweder über den Trigger am Controller oder Blink Detection erfolgen. Mit dem unteren Slider lässt sich der Durchmesser der Buttons in drei Stufen verändern.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{switch-different-options}
	\caption[Einstellungsoberfläche für die Versuchsoptionen]{Einstellungsoberfläche für die Versuchsoptionen; Oben: Anvisieren; Mitte: Bestätigen; Unten: Größe der Buttons}
	\label{fig:switch-different-options}
\end{figure}

Sowohl für die Steuerungsmöglichkeit, als auch für die Skalierung der Buttondurchmesser werden Enumerationen verwendet, die die verschiedenen Status darstellen. Zudem wurde zu jeder Enumeration zusätzlich eine Property-Klasse angelegt. Das Ziel mit der Property-Klasse ist es, die aktuell gültige Steuerungskombination sowie die Skalierung der Buttons global zu sichern. Außerdem soll jede Klasse, die Kenntnisse über die Daten hat, bei Änderungen des Wertes über ein PropertyChanged-Event informiert werden. Die Klassen haben somit bei Interesse an den Werten die Möglichkeit das Event zu abonnieren. \\
Wie in \autoref{fig:ClassDiagrammProperties} zu sehen gibt es die Enumerationen {\ttfamily ControlState} und {\ttfamily Scaling}. ControlState beinhaltet die Enumerationen LaserTrigger, EyeTrigger, LaserBlinking und BlinkingEye. Scaling beinhaltet die Enumerationen Large, Medium und Small. Zu jeder Enumeration existiert zudem die passende Property-Klasse. Während {\ttfamily ControlStateProperty} nur den aktuellen Wert von ControlState speichert, wird in {\ttfamily ScalingProperty} neben dem aktuellen Scaling Wert zusätzlich Faktoren für die Höhe, Breite und Tiefe bei den unterschiedlichen Scalings gespeichert. Der Faktor ist bei Large eins, bei Medium dreiviertel und bei Small nur noch halb so groß.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{ClassDiagramProperties}
	\caption[Klassendiagramm der ScalingProperty \& ControlStateProperty]{Klassendiagramm der ScalingProperty \& ControlStateProperty}
	\label{fig:ClassDiagrammProperties}
\end{figure}

Ein Slider feuert ein On Value Changed Event, sobald sich der Wert des Sliders ändert. In \autoref{LaserEyeTracking} ist die Methode LaserEyeTrackingSliderChanged, welche Änderungen des ersten Sliders zwischen Laser und EyeTracking abfängt. Obwohl in der Methode der neue Wert nicht mitgeliefert werden kann, kann der neue Wert gesetzt werden. Hier wird sich der Eigenschaft einer Enumeration zunutze gemacht, in der der Datentyp des Konstantenwerts der Enumerationsmember ein Integer ist \cite{BillWagner.2020}. Mithilfe von geschickter XOR Arithmetik kann der Status in ControlState gesetzt werden. Da vier Werte vorhanden sind, reichen zwei Bits aus um die Kombinationen zu erstellen. Für das Fokussieren auf ein Objekt wird die niederwertigste Bit-Stelle verwendet. Wenn das Bit 0 ist, wird der Laser zum fokussieren verwendet, bei einer 1 das Eye-Tracking. Bei der höherwertigen Bit-Stelle wird das Bestätigen der Auswahl gesetzt. Eine 0 ist das Verwenden des Abzuges, eine 1 des Blinzelns. Beim Slider für das Scaling werden dem Slider die Werte 0, 1 und 2 zugewiesen. Das aktuelle Scaling als Enumeration wird mittels Typumwandlung von Integer zur Enumeration erhalten.

\begin{lstlisting}[caption=Method LaserEyeTrackingSliderChanged,label=LaserEyeTracking]
public void LaserEyeTrackingSliderChanged()
{
    int state = (int)ControlStateProperty.currentState;
    state = state ^ 0b01;
    this.setControlState((ControlState)state);
}
\end{lstlisting}

\subsection{Buttons}
In den Szenen wird zwischen zwei Arten von Buttons unterschieden. Die erste Art ist der GO-Button, welcher bei Betätigen das Spiel startet. Die zweite Art sind die Spielbuttons, welche mit den Zahlen für das Spiel beschriftet sind. Wie in \autoref{fig:ClassDiagramButton} zu sehen, existieren für beide Arten der Buttons eine Klasse, welche wiederum von der Basisklasse {\ttfamily GeneralButton} abgeleitet sind. Diese Klassen sind den jeweiligen GameObject-Buttons als Komponente zugeordnet. Die Basisklasse stellt die abstrakte Methode DoAction bereit. Diese Methode wird aufgerufen, wenn der Button angeklickt wurde. In der DoStart wird die DoAction als Listener für das Event onClick eines Buttons hinzugefügt. Die beiden Kindklassen ButtonNumber und ButtonGo implementieren die DoAction Methode. Für alle Buttons soll nur eine Aktion passieren, wenn keine Kalibrierung des Eyetrackers läuft. In der DoAction des GO-Buttons wird das Spiel gestartet. Bei der Auswahl eines Spielbuttons, teilt der Button dem Spiel mit, dass dieser Button betätigt wurde. Hierfür wird dem Spiel die Nummer des betätigten Buttons mitgeteilt. Wenn der betätigte Button der gesuchte Button ist, wird die Hintergrundfarbe des Buttons auf grün gesetzt, ansonsten auf rot. Hiermit wird dem Benutzer visuell mitgeteilt, ob der betätigte Button der gesuchte wahr oder nicht. \\
Mithilfe der Klasse {\ttfamily ButtonCanvas} wird die Größe der Spielbuttons verändert. Hierfür wird das PropertyChanged-Event der ScalingProperty abonniert. Bei einer Änderung des Skalierungsfaktors wird die neue Skalierung auf die Buttons angewandt. In ButtonCanvas werden zudem die Buttons zu Beginn und am Ende des 3DLevel an der Wand positioniert. 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{ClassDiagramButton}
	\caption[Klassendiagramm Button]{Klassendiagramm Button}
	\label{fig:ClassDiagramButton}
\end{figure}

Zur Untersuchung von Fitts' Gesetz werden runde Buttons auf der Spielfläche verwendet. Buttons, die von Unity als GameObject zur Verfügung gestellt werden, sind nicht rund, sondern rechteckig. Daher wird für die Spielbuttons das von Unity bereitgestellte GameObject {\ttfamily Cyllinder} verwendet. Ein Problem ist, dass ein Cyllinder standardmäßig keinen Collider beinhaltet. Der Entwickler \citeauthor{kode80.2016} stellt in seinem Repository UnityTools auf Github (siehe \cite{kode80.2016}) eine Lösung für dieses Problem bereit. Er stellt das Asset {\ttfamily CyllinderCollider} inklusive Editor zur Verfügung, welches einen fast runden Collider um einen Cyllinder legt. Der Collider besteht aus einer Anzahl definierter rechteckiger Boxen (n), welche in dem Winkel $\acs{a} = \frac{180^\circ}{n}$ an dem Cyllinder angebracht sind. Die Anzahl der Boxen lässt sich zwischen vier und 64 variieren.\\
In der Abbildung\autoref{fig:CylinderCollider-5} ist der Spielbutton mit dem CyllinderCollider zu sehen. Gut zu sehen sind die fünf rechteckigen Boxen, welche jeweils im Winkel von 36° angebracht sind. Der Collider ist hierbei nicht rund und ist daher minimal größer als der Cyllinder. In der Abbildung\autoref{fig:CylinderCollider-63} wurde die Anzahl der Boxen auf 63 erhöht. Hier passt sich der Collider um einiges besser der Form des Cyllinders an. Während der Implementierung der CyllinderCollider ist aufgefallen, dass eine höhere Anzahl an Boxen eine schlechtere Performance verursacht. Für die Versuche wird daher der CyllinderCollider mit fünf Boxen (Abbildung\autoref{fig:CylinderCollider-5}) verwendet. Für die Versuche ist der Collider mit fünf Boxen ausreichend, da der Benutzer bei den Versuchen in einer gewissen Entfernung steht, weshalb eine minimale Abweichung des Colliders zur Form keine Auswirkungen auf den Versuch haben. Da das System, aufgrund der Anforderungen der verwendeten Hardware, bereits stark ausgelastet ist, wird zudem eine noch höhere Auslastung des Systems vermieden.


\begin{figure}[!htbp]
	\centering
	\subfloat[5 Rechtecke\label{fig:CylinderCollider-5}]{%
		\includegraphics[height=0.45\linewidth]{CylinderCollider_5}
	}
	\qquad      
	\subfloat[63 Rechtecke\label{fig:CylinderCollider-63}]{%
		\includegraphics[height=0.45\linewidth]{CylinderCollider_63}
	}
	\caption{CyllinderCollider}
	\label{fig:CylinderCollider}
\end{figure}  

\subsection{Player}
Das GameObject \glqq Player\grqq{} ist ein zentrales Objekt in den Szenen. Das Objekt beschreibt den Benutzer in Unity und ist über die SteamVR-Bibliothek mit SteamVR verbunden. Das Objekt beinhaltet unter anderem die VR-Kamera und Objekte, welche die rechte, beziehungsweise linke Hand, beschreiben. In den Versuchen wird ein Laserpointer und das Betätigen des Abzuges am Controller verwendet. Hierfür wird der rechten Hand die von SteamVR bereitgestellte Komponente {\ttfamily SteamVR\_LaserPointer} hinzugefügt. Mithilfe dieser Komponente wird der rechten Hand der Laserpointer hinzugefügt. Zudem ist es über diese Komponente möglich das Event PointerClick zu abonnieren, welches beim Betätigen des Abzuges ausgelöst wird. 

Dem Player ist die Klasse {\ttfamily BasePlayer} als Komponente zugeordnet, welche die Steuerungslogik enthält. 
\todo[inline]{Erklärung über ControlState, Scaling, bis hin zu Subscription und Auswahl welche der Daten wann genommen werden.}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{ClassDiagramPlayer}
	\caption[Klassendiagramm Player]{Klassendiagramm Player}
	\label{fig:ClassDiagramPlayer}
\end{figure}

\subsection{Gaze Tracker}
\todo{Gaze Tracker}
Gaze Visualizer??? --> Wird in Subkapitel Measurement benötigt.
BaseMono???

\subsection{Game}
\label{section:game}
%\begin{figure}[!htbp]
%	\centering
%	\includegraphics[width=1\linewidth]{ClassDiagramGame}
%	\caption[Klassendiagramm Game]{Klassendiagramm Game}
%	\label{fig:ClassDiagramGame}
%\end{figure}

Wände haben -1; Buttons die entsprechende Zahl

3DLevel Buttons verschieben

\todo[inline]{Die Zahlen sind nicht zufällig sondern eine zufällige Auswahl von 6 vordefinierten mustern. Die Muster haben alle das gleiche Grundmuster, nur gespiegelt oder versetzt. }

\subsection{Measurement}
\label{section:measurement}
Um einen Vergleich zwischen den Steuerungsmöglichkeiten ziehen zu können, werden die Versuche aufgezeichnet. In \autoref{fig:ClassDiagramMeasurement} ist der Ausschnitt der Klassen für die Measurements zu sehen. Zu jeder Messung wird das aktuelle Level, die verwendete Steuerungskombination sowie die Skalierung der Buttons eines Versuchs gespeichert. Am Anfang sowie am Ende der Messung wird jeweils der aktuelle Zeitpunkt gespeichert. Hierdurch ist es möglich die Dauer des Versuchs zu berechnen. Die Zeit wird in der Messung als Zeitstempel in Millisekunden gesichert. In dem Feld action werden die durch den Benutzer getätigten Bestätigungen durch den Abzug beziehungsweise mithilfe von Blinzeln erfasst. Hierbei wird der Zeitstempel, die Nummer des Buttons, sowie ob es der gesuchte Button war, gespeichert. \todo{Mal schauen wo ich das mit der -1 bei ner Wand erkläre. --> geht hier doch ohne probleme. sollte eine wand getroffen werden, wird als Nummer des Buttons -1 eingetragen. } Die Daten werden als String mit Komma getrennt in einer Liste gespeichert. Für Fitts' Gesetz ist bei der Verwendung von Eye-Tracking der Verlauf des Blickes interessant. Aus diesem Grund wird der Blickverlauf mit gemessen. Die Blickdaten werden ebenfalls in einer Liste bestehend aus Strings gespeichert. Da die Blickdaten als 3D-Vektor vorliegen, werden die Koordinaten des durch den Blick fixierten Punktes mit Komma getrennt aufgezeichnet. Alle 100 ms \todo{ist das sicher 100ms? Würde sagen. Der Eyetracker hat 200 Hz. Nur jede 20 Messung speichern wir ab. Müsste daher 100ms ergeben bzw. 10 Messungen pro Sekunde :D ich meine ich hab da iwas von 20 eingetragen gehabt, weis aber nich ob das jetzt heist alle 20ms oder 20 mal pro sek. Das sollte sich mit den erstellten messungen decken} werden die Blickdaten erfasst und gesichert. Als Datengrundlage für die Blickverfolgung dient der Gaze Visualizer vom Gaze Tracker. Für das 3D-Level werden zusätzlich die Positionen und der Buttons aufgezeichnet. Dies wird benötigt, da sich die Position der Buttons im Raum von Versuch zu Versuch unterscheiden. Für die Zuordnung, welcher Button auf welcher Position steht, werden zusätzlich die Nummern der Buttons mit gespeichert.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.75\linewidth]{ClassDiagramMeasurement}
	\caption[Klassendiagramm Measurements]{Klassendiagramm Measurements}
	\label{fig:ClassDiagramMeasurement}
\end{figure}

Jede Messung wird am Ende eines Versuchs in einer JSON-Datei abgespeichert. Die Basisklasse Measurement ist mit dem Attribut Serializable gekennzeichnet. Dies ermöglicht das Serialisieren des kompletten Measurement-Objekts. Damit alle Messdaten mit dem Objekt serialisiert werden müssen die Felder als öffentlich gekennzeichnet sein. Das Serialisieren in ein JSON-String wird mithilfe von Unity bereitgestellten Klasse JsonUtility durchgeführt. Für eine Gliederung der Messungen werden die Messungen in unterschiedlichen Ordnern nach Level und verwendeter Skalierung getrennt gespeichert. Der Dateiname ist zudem der Zeitstempel zum Zeitpunkt des Speichervorganges der Messung.